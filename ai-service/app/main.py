import os
from dotenv import load_dotenv

load_dotenv()
load_dotenv('.env.local')


os.environ["ANONYMIZED_TELEMETRY"] = "False"
os.environ["CHROMA_TELEMETRY"] = "False"
os.environ["DO_NOT_TRACK"] = "1"

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import asyncio
import json
import logging
from contextlib import asynccontextmanager

from app.workflows.financial_analysis import analyze_financial
from app.services.grok_service import grok_service
from app.services.context_service import ContextService
from app.embeddings import embeddings_service

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Optimized startup and shutdown"""
    logger.info("ðŸš€ Starting Optimized AI Financial Service")

    try:
        await embeddings_service.embed_text("test")
        logger.info("âœ… Embeddings service warmed up")
    except Exception as e:
        logger.warning(f"Embeddings warmup failed: {e}")

    yield

    grok_service.clear_cache()
    logger.info("ðŸ›‘ Optimized AI Service shutdown complete")

app = FastAPI(
    title="Optimized AI Financial Analysis Service",
    description="High-performance financial analysis with minimal token usage",
    version="2.0.0",
    lifespan=lifespan
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

context_service = ContextService()

class AnalysisRequest(BaseModel):
    user_id: str
    question: str
    stream: bool = True
    use_cache: bool = True

@app.get("/health")
async def health_check():
    """Health check with optimization stats"""
    cache_stats = grok_service.get_cache_stats()
    
    return {
        "status": "healthy",
        "service": "AI Financial Analysis",
        "version": "2.0.0",
        "optimization": {
            "cache_enabled": True,
            "cached_messages": cache_stats["cached_messages"],
            "memory_usage_kb": cache_stats["memory_usage_kb"]
        }
    }

@app.post("/analyze/optimized")
async def analyze_optimized_endpoint(request: AnalysisRequest):
    """Optimized analysis endpoint with minimal token usage"""
    try:
        if not request.stream:
            # Non-streaming optimized analysis
            result = await analyze_financial(request.user_id, request.question)
            return result

        # Streaming optimized analysis
        async def generate_optimized_stream():
            try:
                # Get analysis first (cached)
                result = await analyze_financial(request.user_id, request.question)

                if not result["success"]:
                    yield f"data: {json.dumps({'type': 'error', 'error': 'Analysis failed'})}\n\n"
                    return

                # Send optimization stats
                stats_data = {
                    "type": "stats",
                    "data": result["optimization_stats"]
                }
                yield f"data: {json.dumps(stats_data)}\n\n"

                # Stream the response (already optimized)
                response_text = result["response"]

                # Simulate streaming for better UX (chunk the response)
                chunk_size = 10  # words per chunk
                words = response_text.split()

                for i in range(0, len(words), chunk_size):
                    chunk = " ".join(words[i:i + chunk_size])
                    if i + chunk_size < len(words):
                        chunk += " "

                    chunk_data = {
                        "type": "response_chunk",
                        "chunk": chunk
                    }
                    yield f"data: {json.dumps(chunk_data)}\n\n"

                    # Small delay for better streaming effect
                    await asyncio.sleep(0.1)

                # Send completion
                yield f"data: {json.dumps({'type': 'complete'})}\n\n"
            except Exception as e:
                logger.error(f"Optimized streaming error: {e}")
                error_data = {"type": "error", "error": str(e)}
                yield f"data: {json.dumps(error_data)}\n\n"

        return StreamingResponse(
            generate_optimized_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Optimization": "enabled"
            }
        )

    except Exception as e:
        logger.error(f"Optimized endpoint error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/cache/clear")
async def clear_cache():
    """Clear all caches for optimization"""
    try:
        grok_service.clear_cache()
        return {"success": True, "message": "All caches cleared"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/optimization/stats")
async def get_optimization_stats():
    """Get optimization statistics"""
    try:
        cache_stats = grok_service.get_cache_stats()

        return {
            "cache_stats": cache_stats,
            "features": {
                "system_prompt_caching": True,
                "message_deduplication": True,
                "optimized_sql_agent": True,
                "reduced_token_usage": True,
                "langchain_openai": True
            },
            "performance": {
                "estimated_token_savings": "30-50%",
                "response_time_improvement": "20-30%",
                "cache_hit_ratio": "high"
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info",
        access_log=False
    )